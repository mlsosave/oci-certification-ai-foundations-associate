# MÃ³dulo 3: Fundamentos de Deep Learning

## ğŸ“‹ InformaciÃ³n del MÃ³dulo

**Peso en el examen:** 10%  
**Conceptos evaluados:**
- Fundamentos de Deep Learning
- Modelos Convolucionales (CNN)
- Modelos de Secuencia (RNN & LSTM)

---

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar este mÃ³dulo, podrÃ¡s:
1. Explicar quÃ© es Deep Learning y cÃ³mo difiere de ML tradicional
2. Comprender las Redes Neuronales Artificiales (ANN)
3. Entender las CNN para procesamiento de imÃ¡genes
4. Conocer los modelos de secuencia (RNN y LSTM)
5. Identificar aplicaciones de cada arquitectura

---

## 1. Â¿QuÃ© es Deep Learning?

### DefiniciÃ³n
**Deep Learning (DL)** es un subconjunto de Machine Learning que se enfoca en entrenar **redes neuronales artificiales** con muchas capas (profundas) para resolver tareas complejas.

### CaracterÃ­stica Principal
La capacidad de **procesar datos crudos** (como pÃ­xeles de imagen) y **extraer patrones automÃ¡ticamente**, sin necesidad de definir caracterÃ­sticas manualmente.

---

### Ejemplo: Reconocimiento de DÃ­gitos Escritos a Mano

**Problema:**
Cada persona escribe nÃºmeros de manera diferente. Â¿CÃ³mo entrenar una mÃ¡quina para reconocer cualquier dÃ­gito?

```
Diferentes formas de escribir "7":
  7   ã„±   7   â‚‡   7Ì…
```

**SoluciÃ³n con Deep Learning:**

```
Imagen (28Ã—28 pÃ­xeles)
    â†“
Red Neuronal Artificial (ANN)
    â”œâ”€â”€ Detecta bordes
    â”œâ”€â”€ Detecta curvas
    â””â”€â”€ Combina patrones
    â†“
PredicciÃ³n: "7"
```

**Ventaja clave:**
No necesitas decirle a la red quÃ© caracterÃ­sticas buscar. La red **aprende automÃ¡ticamente** quÃ© patrones son importantes.

---

### Deep Learning vs Machine Learning Tradicional

| Aspecto | Machine Learning | Deep Learning |
|---------|------------------|---------------|
| **Features** | Definidas manualmente | ExtraÃ­das automÃ¡ticamente |
| **Datos** | Funciona con pocos datos | Requiere MUCHOS datos |
| **ComputaciÃ³n** | CPU suficiente | Requiere GPU |
| **Interpretabilidad** | MÃ¡s fÃ¡cil de explicar | "Caja negra" |
| **Aplicaciones** | Datos estructurados | ImÃ¡genes, audio, texto |

---

### Historia de Deep Learning

```
1950s: Concepto de neurona artificial
  â†“
1980s: Backpropagation (ajuste de pesos)
  â†“
1990s: CNN para anÃ¡lisis de imÃ¡genes
  â†“
2000s: GPU se vuelven mÃ¡s potentes y baratas
  â†“
2010s: GPU ampliamente disponibles
  â”œâ”€â”€ 2012: AlexNet (ImageNet)
  â”œâ”€â”€ 2016: Modelos generativos
  â””â”€â”€ 2020+: LLMs (ChatGPT, GPT-4)
```

---

## 2. Redes Neuronales Artificiales (ANN)

### InspiraciÃ³n: El Cerebro Humano

**Neurona biolÃ³gica:**
```
Dendritas â†’ NÃºcleo â†’ AxÃ³n â†’ Sinapsis
(Entrada)   (Procesa) (Salida) (ConexiÃ³n)
```

**Neurona artificial:**
```
Inputs â†’ Pesos â†’ Suma â†’ ActivaciÃ³n â†’ Output
(xâ‚,xâ‚‚)   (wâ‚,wâ‚‚)   (Î£)     (f)      (y)
```

---

### Componentes de una ANN

#### 1. Capas (Layers)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Layer  â”‚ â† Datos de entrada
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hidden Layer â”‚ â† Procesamiento
â”‚ Hidden Layer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer â”‚ â† Resultado
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tipos de capas:**
- **Input Layer (Entrada):** Recibe los datos (OBLIGATORIA)
- **Hidden Layers (Ocultas):** Procesan informaciÃ³n (OPCIONALES, mÃºltiples)
- **Output Layer (Salida):** Produce el resultado (OBLIGATORIA)

---

#### 2. Neuronas

Cada neurona:
1. Recibe mÃºltiples entradas
2. Las multiplica por pesos
3. Suma todo
4. Aplica funciÃ³n de activaciÃ³n
5. Produce una salida

**Ejemplo matemÃ¡tico:**
```
Inputs: xâ‚ = 2, xâ‚‚ = 3
Pesos: wâ‚ = 0.5, wâ‚‚ = 0.3
Bias: b = 1

Suma ponderada:
z = (xâ‚ Ã— wâ‚) + (xâ‚‚ Ã— wâ‚‚) + b
z = (2 Ã— 0.5) + (3 Ã— 0.3) + 1
z = 1 + 0.9 + 1 = 2.9

ActivaciÃ³n (ReLU):
output = max(0, z) = 2.9
```

---

#### 3. Pesos (Weights)

Los **pesos** determinan la importancia de cada conexiÃ³n.

**AnalogÃ­a:**
En un examen:
- Pregunta 1 tiene peso 0.3 (30%)
- Pregunta 2 tiene peso 0.7 (70%)

Si sacas 80% en ambas:
```
Nota final = (80 Ã— 0.3) + (80 Ã— 0.7) = 80%
```

Pero si:
- Pregunta 1: 100%
- Pregunta 2: 60%

```
Nota final = (100 Ã— 0.3) + (60 Ã— 0.7) = 72%
```

Los pesos dan mÃ¡s importancia a ciertas entradas.

---

#### 4. Funciones de ActivaciÃ³n

Deciden si una neurona debe "activarse" o no.

**Funciones comunes:**

| FunciÃ³n | FÃ³rmula | Uso |
|---------|---------|-----|
| **ReLU** | f(x) = max(0, x) | Capas ocultas (mÃ¡s comÃºn) |
| **Sigmoid** | f(x) = 1/(1+e^-x) | ClasificaciÃ³n binaria |
| **Tanh** | f(x) = (e^x - e^-x)/(e^x + e^-x) | Datos centrados en 0 |
| **Softmax** | Normaliza probabilidades | Multi-clase |

**GrÃ¡fica ReLU:**
```
    y
    â”‚     â•±
    â”‚    â•±
    â”‚   â•±
â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â†’ x
    â”‚
```

---

#### 5. Bias

El **bias** permite flexibilidad al modelo, similar a la "constante" en una ecuaciÃ³n.

**Sin bias:**
```
y = 2x
```
Solo puede pasar por el origen (0,0)

**Con bias:**
```
y = 2x + 3
```
Puede desplazarse verticalmente

---

### Ejemplo Completo: Reconocer DÃ­gitos

**Arquitectura:**

```
Input: 28Ã—28 = 784 pÃ­xeles
    â†“
Hidden Layer 1: 16 neuronas
    â†“
Hidden Layer 2: 16 neuronas
    â†“
Output: 10 neuronas (0-9)
```

**Proceso:**
1. Imagen de "2" entra como 784 nÃºmeros
2. Capas ocultas detectan patrones (curvas, bordes)
3. Capa de salida predice: neurona #2 se activa

---

### Entrenamiento: Backpropagation

**Problema:**
Â¿CÃ³mo ajustamos los pesos para que la red aprenda?

**SoluciÃ³n: Backpropagation**

```
1. Forward Pass (PredicciÃ³n)
   Imagen â†’ Red â†’ PredicciÃ³n: "6"
   
2. Calcular Error
   Real: "2"
   Predicho: "6"
   Error = grande âŒ
   
3. Backward Pass
   Ajustar pesos para reducir error
   
4. Repetir miles de veces
   Imagen â†’ Red â†’ PredicciÃ³n: "2" âœ…
```

**AnalogÃ­a:**
Como practicar tiros libres en baloncesto:
1. Tiras y fallas
2. Ajustas tu tÃ©cnica
3. Tiras de nuevo
4. Repites hasta acertar consistentemente

---

## 3. Modelos de Secuencia (RNN y LSTM)

### Â¿QuÃ© son Secuencias?

**Secuencias** son datos donde el **orden importa**.

**Ejemplos:**
- Texto: "El perro come" â‰  "Come el perro"
- Audio: Serie de sonidos en el tiempo
- Video: Secuencia de frames
- Series de tiempo: Precios de acciones

---

### Aplicaciones de Modelos de Secuencia

| AplicaciÃ³n | Input â†’ Output |
|------------|----------------|
| **TraducciÃ³n** | Texto en inglÃ©s â†’ Texto en francÃ©s |
| **Reconocimiento de voz** | Audio â†’ Texto |
| **GeneraciÃ³n de mÃºsica** | Notas previas â†’ Siguiente nota |
| **PredicciÃ³n de clima** | Datos histÃ³ricos â†’ Temperatura futura |
| **Lenguaje de seÃ±as** | Secuencia de gestos â†’ Texto |

---

### Recurrent Neural Networks (RNN)

**CaracterÃ­stica principal:**
RNN tienen **memoria**. Cada paso depende del anterior.

**Arquitectura:**

```
Entrada:    xâ‚  â†’  xâ‚‚  â†’  xâ‚ƒ  â†’  xâ‚„
             â†“      â†“      â†“      â†“
Estado:    [hâ‚] â†’ [hâ‚‚] â†’ [hâ‚‚] â†’ [hâ‚„]
             â†“      â†“      â†“      â†“
Salida:     yâ‚     yâ‚‚     yâ‚ƒ     yâ‚„
```

**Flujo de informaciÃ³n:**
- Datos fluyen de izquierda a derecha
- Cada paso comparte informaciÃ³n con el siguiente

---

### Tipos de Arquitecturas RNN

#### 1. One-to-One
```
1 Input â†’ 1 Output
```
**No** es realmente secuencial. Es como una red normal.

---

#### 2. One-to-Many
```
1 Input â†’ MÃºltiples Outputs
```
**Ejemplo:** GeneraciÃ³n de mÃºsica
```
Input: Nota inicial "Do"
Output: Do â†’ Re â†’ Mi â†’ Fa â†’ Sol
```

---

#### 3. Many-to-One
```
MÃºltiples Inputs â†’ 1 Output
```
**Ejemplo:** AnÃ¡lisis de sentimiento
```
Input: "La pelÃ­cula fue excelente y emocionante"
Output: Sentimiento = Positivo
```

---

#### 4. Many-to-Many
```
MÃºltiples Inputs â†’ MÃºltiples Outputs
```
**Ejemplo:** TraducciÃ³n automÃ¡tica
```
Input: "How are you?"
Output: "Â¿CÃ³mo estÃ¡s?"
```

---

### Problema de RNN: Vanishing Gradient

**LimitaciÃ³n:**
RNN tienen **mala memoria a largo plazo**. Olvidan informaciÃ³n de pasos muy anteriores.

**Ejemplo:**
```
OraciÃ³n: "El gato, que vive en la casa azul de mi abuela, come pescado"

Pregunta: Â¿QuiÃ©n come pescado?
```

RNN puede olvidar que estamos hablando del "gato" porque hay mucho texto en el medio.

---

### Long Short-Term Memory (LSTM)

**SoluciÃ³n al problema de RNN:**
LSTM puede **recordar informaciÃ³n a largo plazo** mediante "compuertas" (gates).

---

### CÃ³mo Funciona LSTM

**Componentes principales:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LSTM Cell         â”‚
â”‚                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Forget Gate     â”‚ â”‚ Â¿QuÃ© olvidar?
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Input Gate      â”‚ â”‚ Â¿QuÃ© recordar?
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Output Gate     â”‚ â”‚ Â¿QuÃ© compartir?
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 1. Forget Gate (Compuerta de Olvido)
**Decide quÃ© informaciÃ³n eliminar de la memoria.**

**Ejemplo:**
```
Texto: "El gato es negro. El perro es blanco."
```
Cuando llega "El perro", puede olvidar informaciÃ³n sobre el gato.

---

#### 2. Input Gate (Compuerta de Entrada)
**Decide quÃ© informaciÃ³n nueva agregar a la memoria.**

**Ejemplo:**
```
"El perro es blanco"
```
Guarda: color del perro = blanco

---

#### 3. Output Gate (Compuerta de Salida)
**Decide quÃ© informaciÃ³n mostrar como salida.**

**Ejemplo:**
```
Pregunta: "Â¿QuÃ© color es el perro?"
```
Output Gate recupera: "blanco"

---

### Proceso LSTM Paso a Paso

```
1. Input actual + Estado oculto anterior
   â†“
2. Forget Gate decide quÃ© olvidar
   â†“
3. Input Gate decide quÃ© recordar
   â†“
4. Actualizar Cell State (memoria)
   â†“
5. Output Gate decide quÃ© mostrar
   â†“
6. Generar output y nuevo estado oculto
```

---

### RNN vs LSTM

| CaracterÃ­stica | RNN | LSTM |
|----------------|-----|------|
| **Memoria** | Corto plazo | Largo plazo |
| **Complejidad** | Simple | MÃ¡s complejo |
| **Velocidad** | MÃ¡s rÃ¡pido | MÃ¡s lento |
| **Vanishing Gradient** | SÃ­ âŒ | No âœ… |
| **Aplicaciones** | Secuencias cortas | Secuencias largas |

---

## 4. Convolutional Neural Networks (CNN)

### Â¿Para QuÃ© se Usan las CNN?

**CNN** son especializadas en procesar **imÃ¡genes** y **videos**.

**Ventaja principal:**
Detectan automÃ¡ticamente patrones visuales como:
- Bordes
- Texturas
- Formas
- Objetos

---

### Problema con Redes Neuronales Normales para ImÃ¡genes

**Imagen de 28Ã—28 pÃ­xeles:**
```
Red normal:
- Convierte imagen a vector 1D: 784 nÃºmeros
- Pierde informaciÃ³n espacial
- No aprovecha que pÃ­xeles cercanos estÃ¡n relacionados
```

**CNN:**
```
- Procesa imagen en 2D
- Mantiene relaciones espaciales
- Detecta patrones locales
```

---

### Arquitectura CNN

```
Input Image
    â†“
Convolutional Layers
    â†“
Pooling Layers
    â†“
Flatten
    â†“
Fully Connected Layers
    â†“
Output (ClasificaciÃ³n)
```

---

### Componentes de CNN

#### 1. Convolutional Layer (Capa Convolucional)

**FunciÃ³n:**
Detecta caracterÃ­sticas mediante **filtros** (kernels).

**Ejemplo: Detector de bordes verticales**

```
Filtro 3Ã—3:
[-1  0  1]
[-1  0  1]
[-1  0  1]
```

Este filtro desliza sobre la imagen detectando bordes verticales.

**Proceso:**
```
Imagen original:
[10 10 10 | 50 50 50]
[10 10 10 | 50 50 50]
[10 10 10 | 50 50 50]

Aplicar filtro:
[0  0  | 40 40]
[0  0  | 40 40]
```

Â¡El filtro detectÃ³ el borde vertical! âœ…

---

#### 2. Activation Function (ReLU)

DespuÃ©s de convoluciÃ³n, se aplica **ReLU** para no linealidad.

```
ReLU(x) = max(0, x)
```

**Efecto:**
- Valores positivos â†’ Mantener
- Valores negativos â†’ 0

---

#### 3. Pooling Layer (Capa de Agrupamiento)

**FunciÃ³n:**
**Reducir dimensiones** manteniendo informaciÃ³n importante.

**Max Pooling 2Ã—2:**
```
Entrada:
[2  8  | 3  1]
[5  1  | 7  2]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[4  6  | 9  3]
[0  2  | 1  5]

Max Pooling:
[8  7]
[6  9]
```

**Ventajas:**
- â¬‡ï¸ Reduce tamaÃ±o
- ğŸš€ Menos computaciÃ³n
- ğŸ›¡ï¸ Reduce overfitting

---

#### 4. Flatten

**Convierte matriz 2D â†’ vector 1D**

```
Entrada (2Ã—2):
[8  7]
[6  9]

Flatten:
[8, 7, 6, 9]
```

---

#### 5. Fully Connected Layers

**Red neuronal normal** que hace la clasificaciÃ³n final.

```
[8, 7, 6, 9] â†’  Dense Layer â†’ Output: "Gato"
```

---

### AnalogÃ­a: Robot Inspector de Casas

**Herramienta 1: Detector de planos (Convolutional Layer)**
- Escanea partes de la casa
- Busca patrones especÃ­ficos

**Herramienta 2: Marcador de patrones (Activation)**
- Resalta Ã¡reas detectadas

**Herramienta 3: Resumidor (Pooling)**
- Captura caracterÃ­sticas mÃ¡s importantes de cada habitaciÃ³n

**Herramienta 4: Experto en casas (Fully Connected)**
- Analiza todos los patrones
- Decide quÃ© tipo de casa es

**Herramienta 5: Adivinador (Softmax)**
- Asigna probabilidades a tipos de casa

**Herramienta 6: Control de calidad (Dropout)**
- Evita confiar demasiado en un solo patrÃ³n

---

### Aplicaciones de CNN

| AplicaciÃ³n | DescripciÃ³n |
|------------|-------------|
| **ClasificaciÃ³n de imÃ¡genes** | Â¿Es un gato o un perro? |
| **DetecciÃ³n de objetos** | DÃ³nde estÃ¡n los objetos (bounding boxes) |
| **SegmentaciÃ³n** | Etiquetar cada pÃ­xel (cielo, Ã¡rbol, calle) |
| **Reconocimiento facial** | Identificar personas |
| **ImÃ¡genes mÃ©dicas** | DetecciÃ³n de tumores |
| **Autos autÃ³nomos** | Reconocer seÃ±ales de trÃ¡nsito, peatones |
| **AnÃ¡lisis satelital** | ClasificaciÃ³n de uso de tierra |

---

### Limitaciones de CNN

| LimitaciÃ³n | DescripciÃ³n |
|------------|-------------|
| **Computacionalmente costosas** | Requieren GPU potentes |
| **Necesitan muchos datos** | Miles de imÃ¡genes etiquetadas |
| **Overfitting** | Si hay pocos datos |
| **Caja negra** | DifÃ­cil interpretar decisiones |
| **Sensibles a cambios** | PequeÃ±os cambios pueden causar errores |

---

## 5. Arquitecturas de Deep Learning - Resumen

| Arquitectura | Datos | AplicaciÃ³n |
|--------------|-------|------------|
| **FNN** (Feedforward) | Tabulares | PredicciÃ³n de precios |
| **CNN** | ImÃ¡genes, videos | ClasificaciÃ³n de imÃ¡genes |
| **RNN** | Secuencias | PredicciÃ³n de series de tiempo |
| **LSTM** | Secuencias largas | TraducciÃ³n, anÃ¡lisis de sentimiento |
| **Autoencoders** | Cualquier tipo | CompresiÃ³n, anomalÃ­as |
| **GAN** | ImÃ¡genes | GeneraciÃ³n de imÃ¡genes realistas |
| **Transformers** | Texto | ChatGPT, traducciÃ³n |

---

## ğŸ“ Preguntas de PrÃ¡ctica para el Examen

### Pregunta 1
**Â¿QuÃ© modelo de secuencia puede mantener informaciÃ³n relevante en secuencias largas?**

- â—‹ Redes Neuronales Recurrentes
- âœ… Redes Neuronales de Memoria a Corto y Largo Plazo (LSTM)
- â—‹ Redes Neuronales Feedforward
- â—‹ Redes Neuronales Convolucionales

**ExplicaciÃ³n:** LSTM estÃ¡ diseÃ±ado especÃ­ficamente para manejar dependencias a largo plazo, resolviendo el problema del vanishing gradient.

---

### Pregunta 2
**Â¿QuÃ© red neuronal tiene un bucle de retroalimentaciÃ³n y estÃ¡ diseÃ±ada para manejar datos secuenciales?**

- â—‹ Redes Neuronales Feedforward
- âœ… Redes Neuronales Recurrentes
- â—‹ Redes Neuronales Convolucionales
- â—‹ Redes Neuronales de PerceptrÃ³n Multicapa

**ExplicaciÃ³n:** RNN tienen conexiones de retroalimentaciÃ³n que permiten procesar secuencias.

---

### Pregunta 3
**Â¿QuÃ© componente esencial de una Red Neuronal Artificial realiza suma ponderada y aplica funciÃ³n de activaciÃ³n?**

- â—‹ Iterador
- â—‹ Bias
- âœ… Neurona
- â—‹ Clasificador

**ExplicaciÃ³n:** La neurona es la unidad fundamental que procesa informaciÃ³n.

---

## ğŸ“ Consejos para el Examen

### Diferencias Clave

| Modelo | Datos | Memoria | AplicaciÃ³n |
|--------|-------|---------|------------|
| **CNN** | ImÃ¡genes | No | ClasificaciÃ³n de imÃ¡genes |
| **RNN** | Secuencias | Corto plazo | Series de tiempo cortas |
| **LSTM** | Secuencias | Largo plazo | TraducciÃ³n, NLP |

### Arquitecturas - Resumen RÃ¡pido

**Pregunta del examen: Â¿QuÃ© modelo usar para...?**

- ğŸ“¸ ImÃ¡genes â†’ **CNN**
- ğŸµ MÃºsica/Texto â†’ **RNN o LSTM**
- ğŸ“ˆ PredicciÃ³n numÃ©rica â†’ **FNN**
- ğŸ¨ Generar imÃ¡genes â†’ **GAN**

### Componentes de ANN

| Componente | FunciÃ³n |
|------------|---------|
| **Capas** | Organizan neuronas (input, hidden, output) |
| **Neuronas** | Unidades de procesamiento |
| **Pesos** | Importancia de conexiones |
| **Activation** | Decide si neurona se activa |
| **Bias** | Flexibilidad del modelo |

---

## ğŸ“š Resumen del MÃ³dulo

| Concepto | Puntos Clave |
|----------|--------------|
| **Deep Learning** | Redes neuronales profundas, mÃºltiples capas |
| **ANN** | Neuronas conectadas en capas |
| **Backpropagation** | Ajuste de pesos para aprender |
| **CNN** | Especializada en imÃ¡genes, usa filtros |
| **RNN** | Memoria a corto plazo, datos secuenciales |
| **LSTM** | Memoria a largo plazo, compuertas |

---

## âœ… Checklist de PreparaciÃ³n

Antes de avanzar, asegÃºrate de poder:

- [ ] Explicar la diferencia entre ML y DL
- [ ] Describir los componentes de una ANN
- [ ] Entender cÃ³mo funciona backpropagation
- [ ] Identificar cuÃ¡ndo usar CNN vs RNN
- [ ] Explicar el problema del vanishing gradient
- [ ] Describir las compuertas de LSTM
- [ ] Dar ejemplos de aplicaciones de cada arquitectura

